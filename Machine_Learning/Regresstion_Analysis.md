## Regresstion Analysis (회귀 분석)

회귀 분석의 목적은 독립변수(입력 데이터,x)가 주어졌을 때 종속변수(출력 데이터,y)와의 관계를

추정하기 위한 분석 방법이다. Supervised Learning의 한 부분이다.

## Linear Regression ( 선형 회귀 분석)

가장 일반적인 예시로 **y = ax + b ** x축과 y축을 가로지르는 직선(분류선,기울기)을 기준으로 어떤 데이터가

주어졌을 때 e(오차)가 제일 적은 곳을 기준으로 결과 데이터가 나오게 된다.

마지막 데이터의 영향을 많이 받으므로 학습률(Learning Rate)을 이용하는 것이 좋다.

또한 영향을 많이 받는다는 것은 기울기가 계속 변한다는 것으로 이항분류에서는 적합하지 않다.

선형 회귀 분석 또한 Supervised Learning의 한 부분으로 데이터가 주어지면 컴퓨터는 계속해서

학습을 하고 있는 형태인 것 같다.

## Logistic Regression ( 로지스틱 회귀 분석)

**Sigmoid Function** 이라고도 불리운다.

**범주형(카테고리)**으로 표현되는 결과를 얻고 싶을 때 쓴다.(이항 분류)

선형 회귀 분석의 큰 문제점인 기울기가 변하는 것을 방지하기 위해 쓰는 적합한 함수가 로지스틱이라

볼 수 있다. 식을 구하기 위해 y = ax + b 선형 함수를 불러와 y에 확률 p를 대입하고 양 쪽의 값의 범위를

맞추기 위해 odds(승산)을 대입하고 odds에 log까지 씌우면 양 쪽 범위가 일치하게 된다.

그 이후의 유도식은 인터넷을 보도록하자.

대충 y = x / 1 + x 이런 이차원 함수 그래프가 로지스틱 그래프와 거의 유사하다고 볼 수 있다.

0에도 1에도 수렴하지않지만 무한적인 수가 존재하는 그래프 형식을 띄게 된다.

그래서 결과를 0 또는 1로 구분지을 수 있는 상황에 이용한다.(주로 이항분류 같은?)

선형 회귀의 문제점을 보완해서 이 둘을 연결지어 생각하면 안되고 서로 쓰임새가 다르기에

독립적이라고 봐야한다.

