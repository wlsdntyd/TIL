## Regresstion Analysis (회귀 분석)

회귀 분석의 목적은 독립변수(입력 데이터,x)가 주어졌을 때 종속변수(출력 데이터,y)와의 관계를

추정하기 위한 분석 방법이다. Supervised Learning의 한 부분이다.

## Linear Regression ( 선형 회귀 분석)

가장 일반적인 예시로 **y = ax + b ** x축과 y축을 가로지르는 직선(분류선,기울기)을 기준으로 어떤 데이터가

주어졌을 때 e(오차)가 제일 적은 곳을 기준으로 결과 데이터가 나오게 된다.

마지막 데이터의 영향을 많이 받으므로 학습률(Learning Rate)을 이용하는 것이 좋다.

또한 영향을 많이 받는다는 것은 기울기가 계속 변한다는 것으로 이항분류에서는 적합하지 않다.

선형 회귀 분석 또한 Supervised Learning의 한 부분으로 데이터가 주어지면 컴퓨터는 계속해서

학습을 하고 있는 형태인 것 같다.

오차를 구하는 방법 : 

- 어떠한 점을 기준으로 기울기를 직각으로 가로지르는 선(함수)을 만들어 두 점 사이의

거리를 구하는 방식, 두 직선을 이루는 기울기의 곱은 -1이어야 한다.

>  1)  y = 2x + 1 에 직각을 이루는 선은 2) y = -1/2x + b 이며 임의의 점(D)이 (4,3)일시 임의의 점을 지나는
>
> 선 2) 에 D를 대입하면 3 = -2 + b 로 y축 절편 b는 5가 된다. 그럼 두 선이 교차되는 지점을 기준으로
>
> y = 2x + 1 = -1/2x + 5 가 성립이 된다. x를 중심으로 뒤에 식 두개를 계산할 시 x는 8/5 이라는 값이
>
> 나오고 y는 21/5 값이 나온다. 그럼 두 점 사이의 거리는 ((4 - 8/5)² + (3 - 21/5)²)½ = e(오차) 가 된다.

- 그냥 y축을 중심으로 (|) 형태의 거리를 계산하는 방식도 있다.

>   y = ax + b 의 선형 회귀 함수가 주어졌을 때 임의의 점 (i, k) 까지의 |형태의 오차를 구하고 싶다면
>
> 결과적으로는 k - y 이며 과정은 대략 이러하다.
>
> y = ai + b 의 좌표는 (i, ai + b) 그럼 임의의 점과 기울기의 좌표의 거리는 k - (ai + b) 이며
>
> 좀 더 보기 좋게 쓰자면 |(ai + b) - k| 이렇게 쓸 수 있다. 절대값을 해줘야 결과가 정확하다.

## Logistic Regression ( 로지스틱 회귀 분석)

**Sigmoid Function** 과 많이 유사하다.

**범주형(카테고리)**으로 표현되는 결과를 얻고 싶을 때 쓴다.(이항 분류)

선형 회귀 분석의 큰 문제점인 기울기가 변하는 것을 방지하기 위해 쓰는 적합한 함수가 로지스틱이라

볼 수 있다. 식을 구하기 위해 y = ax + b 선형 함수를 불러와 y에 확률 p를 대입하고 양 쪽의 값의 범위를

맞추기 위해 odds(승산)을 대입하고 odds에 log까지 씌우면 양 쪽 범위가 일치하게 된다.

그 이후의 유도식은 인터넷을 보도록하자.

대충 y = x / 1 + x 이런 일차원 함수 그래프가 로지스틱 그래프와 거의 유사하다고 볼 수 있다.

0에도 1에도 수렴하지않지만 x에 무한적인 수가 존재하는 그래프 형식을 띄게 된다.

그래서 결과를 0 또는 1로 구분지을 수 있는 상황에 이용한다.(주로 이항분류 같은?)

선형 회귀의 문제점을 보완해서 이 둘을 연결지어 생각하면 안되고 서로 쓰임새가 다르기에

독립적이라고 봐야한다.

